{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10a38fe3-807e-49ae-a3b7-a2030afb6f70",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create catalog, schemas, volumes"
    }
   },
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# COMPLETE CLEANUP + SETUP FOR MEDALLION PIPELINE\n",
    "# Run this cell FIRST (and only once, or whenever you want a fresh start)\n",
    "# =============================================\n",
    "\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "print(\"Starting full cleanup and setup of Unity Catalog structure...\\n\")\n",
    "\n",
    "# Step 1: Drop the entire catalog if it exists (complete cleanup)\n",
    "try:\n",
    "    spark.sql(\"DROP CATALOG IF EXISTS it_ticket_medallion CASCADE\")\n",
    "    print(\"Existing catalog 'it_ticket_medallion' dropped (including all schemas, volumes, and data)\")\n",
    "except AnalysisException as e:\n",
    "    print(\"No existing catalog to drop or already clean\")\n",
    "\n",
    "# Step 2: Re-create everything fresh\n",
    "print(\"\\nCreating new catalog, schemas, and volumes...\")\n",
    "\n",
    "spark.sql(\"CREATE CATALOG IF NOT EXISTS it_ticket_medallion\")\n",
    "print(\"Catalog created: it_ticket_medallion\")\n",
    "\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS it_ticket_medallion.bronze\")\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS it_ticket_medallion.silver\")\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS it_ticket_medallion.gold\")\n",
    "print(\"Schemas created: bronze, silver, gold\")\n",
    "\n",
    "spark.sql(\"CREATE VOLUME IF NOT EXISTS it_ticket_medallion.bronze.raw_delta\")\n",
    "spark.sql(\"CREATE VOLUME IF NOT EXISTS it_ticket_medallion.silver.clean_delta\")\n",
    "spark.sql(\"CREATE VOLUME IF NOT EXISTS it_ticket_medallion.gold.aggregated_delta\")\n",
    "spark.sql(\"CREATE VOLUME IF NOT EXISTS it_ticket_medallion.gold.ticket_aggregates_csv\")\n",
    "print(\"Volumes created:\")\n",
    "print(\"  - bronze.raw_delta\")\n",
    "print(\"  - silver.clean_delta\")\n",
    "print(\"  - gold.aggregated_delta\")\n",
    "print(\"  - gold.ticket_aggregates_csv (for CSV export)\")\n",
    "\n",
    "print(\"\\nUnity Catalog structure is now clean and ready!\")\n",
    "print(\"You can now safely run the Bronze → Silver → Gold pipeline cells multiple times.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "672a7cb4-147a-4f6c-9081-bb40c95ff8a8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Read Raw Data"
    }
   },
   "outputs": [],
   "source": [
    "# Read Raw Data\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Workspace path\n",
    "csv_path = \"/Workspace/Users/avi@aidamant.com/servicenow-pipeline/servicenow_incidents_10k.csv\"\n",
    "\n",
    "# Read with Pandas\n",
    "pandas_df = pd.read_csv(csv_path)\n",
    "\n",
    "# Convert to Spark DataFrame\n",
    "raw_df = spark.createDataFrame(pandas_df)\n",
    "\n",
    "# Print row count, schema, first 20 records\n",
    "print(f\"Raw row count: {raw_df.count()}\")\n",
    "raw_df.printSchema()\n",
    "display(raw_df.limit(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a3a4581-2a08-4b8f-a953-6b8e40351d39",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Bronze Layer"
    }
   },
   "outputs": [],
   "source": [
    "# Bronze Layer\n",
    "bronze_volume_path = \"/Volumes/it_ticket_medallion/bronze/raw_delta\"\n",
    "\n",
    "print(\"Overwriting Bronze layer...\")\n",
    "# Forces ~4 files\n",
    "raw_df.repartition(4).write.format(\"delta\").mode(\"overwrite\").save(bronze_volume_path)\n",
    "print(\"✓ Bronze overwritten successfully\")\n",
    "\n",
    "print(f\"Bronze row count: {spark.read.format('delta').load(bronze_volume_path).count()}\")\n",
    "\n",
    "print(\"✓ Bronze Delta in volume\")\n",
    "display(spark.read.format(\"delta\").load(bronze_volume_path).limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62658d1a-bdcb-4e8b-9644-40faeef51d33",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Silver Layer"
    }
   },
   "outputs": [],
   "source": [
    "# Silver Layer\n",
    "from pyspark.sql.functions import col, to_timestamp\n",
    "\n",
    "bronze_df = spark.read.format(\"delta\").load(bronze_volume_path)\n",
    "\n",
    "# Normalization + cleaning\n",
    "silver_df = bronze_df.select([col(c).alias(c.lower().replace(\" \", \"_\").replace(\".\", \"_\").replace(\"-\", \"_\")) for c in bronze_df.columns])\n",
    "\n",
    "silver_df = silver_df.filter(col(\"number\").isNotNull())\n",
    "\n",
    "silver_df = silver_df.withColumn(\"opened_at\", to_timestamp(col(\"opened_at\"), \"M/d/yyyy H:mm\")) \\\n",
    "                    .withColumn(\"closed_at\", to_timestamp(col(\"closed_at\"), \"M/d/yyyy H:mm\"))\n",
    "\n",
    "numeric_cols = [\"priority\", \"impact\", \"urgency\"]\n",
    "for c in numeric_cols:\n",
    "    if c in silver_df.columns:\n",
    "        silver_df = silver_df.withColumn(c, col(c).cast(\"int\"))\n",
    "\n",
    "print(\"Overwriting Silver layer...\")\n",
    "silver_volume_path = \"/Volumes/it_ticket_medallion/silver/clean_delta\"\n",
    "silver_df.write.format(\"delta\").mode(\"overwrite\").save(silver_volume_path)\n",
    "print(\"✓ Silver overwritten successfully\")\n",
    "\n",
    "print(f\"Silver row count: {spark.read.format('delta').load(silver_volume_path).count()}\")\n",
    "\n",
    "display(silver_df.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "873c9ca3-25e5-4bf8-aaa6-e0651dde6f46",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Gold Layer"
    }
   },
   "outputs": [],
   "source": [
    "# Gold Layer\n",
    "silver_df = spark.read.format(\"delta\").load(silver_volume_path)\n",
    "\n",
    "gold_df = silver_df.groupBy(\"state\", \"priority\").count() \\\n",
    "                   .withColumnRenamed(\"count\", \"ticket_count\") \\\n",
    "                   .orderBy(col(\"ticket_count\").desc())\n",
    "\n",
    "print(\"Overwriting Gold layer...\")\n",
    "gold_volume_path = \"/Volumes/it_ticket_medallion/gold/aggregated_delta\"\n",
    "gold_df.write.format(\"delta\").mode(\"overwrite\").save(gold_volume_path)\n",
    "print(\"✓ Gold Delta layer saved\")\n",
    "\n",
    "print(f\"Gold row count: {spark.read.format('delta').load(gold_volume_path).count()}\")\n",
    "\n",
    "display(gold_df)\n",
    "\n",
    "# Export Gold as single CSV into the dedicated volume\n",
    "gold_csv_volume_path = \"/Volumes/it_ticket_medallion/gold/ticket_aggregates_csv\"\n",
    "\n",
    "gold_df.coalesce(1).write \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .csv(gold_csv_volume_path)\n",
    "\n",
    "print(f\"✓ Gold CSV exported successfully!\")\n",
    "print(f\"Location: {gold_csv_volume_path}\")\n",
    "print(\"Find & download in Catalog Explorer:\")\n",
    "print(\"   it_ticket_medallion → gold → ticket_aggregates_csv → part-00000-*.csv\")\n",
    "print(\"   Right-click the CSV file → Download\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "it_ticket_pipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
